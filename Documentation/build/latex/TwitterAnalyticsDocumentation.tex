%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
 \ifdefined\DeclareUnicodeCharacterAsOptional
  \DeclareUnicodeCharacter{"00A0}{\nobreakspace}
  \DeclareUnicodeCharacter{"2500}{\sphinxunichar{2500}}
  \DeclareUnicodeCharacter{"2502}{\sphinxunichar{2502}}
  \DeclareUnicodeCharacter{"2514}{\sphinxunichar{2514}}
  \DeclareUnicodeCharacter{"251C}{\sphinxunichar{251C}}
  \DeclareUnicodeCharacter{"2572}{\textbackslash}
 \else
  \DeclareUnicodeCharacter{00A0}{\nobreakspace}
  \DeclareUnicodeCharacter{2500}{\sphinxunichar{2500}}
  \DeclareUnicodeCharacter{2502}{\sphinxunichar{2502}}
  \DeclareUnicodeCharacter{2514}{\sphinxunichar{2514}}
  \DeclareUnicodeCharacter{251C}{\sphinxunichar{251C}}
  \DeclareUnicodeCharacter{2572}{\textbackslash}
 \fi
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\usepackage{geometry}

% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}
\addto\captionsenglish{\renewcommand{\contentsname}{Contents:}}

\addto\captionsenglish{\renewcommand{\figurename}{Fig.}}
\addto\captionsenglish{\renewcommand{\tablename}{Table}}
\addto\captionsenglish{\renewcommand{\literalblockname}{Listing}}

\addto\captionsenglish{\renewcommand{\literalblockcontinuedname}{continued from previous page}}
\addto\captionsenglish{\renewcommand{\literalblockcontinuesname}{continues on next page}}

\addto\extrasenglish{\def\pageautorefname{page}}

\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}


\title{Twitter Analytics Documentation}
\date{Jun 11, 2018}
\release{0.1}
\author{Deepak Saini, Abhishek Gupta}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex

\begin{document}

\maketitle
\sphinxtableofcontents
\phantomsection\label{\detokenize{index::doc}}


Twitter generates millions of tweets each day. A vast amount of information is hence available for different kinds of analyses. The end users of these analyses however, may or may not be technically proficient. This necessitates the need of a system that can absorb such large amounts of data and at the same time, provide an intuitive abstraction over this data. This abstraction should allow the end users to specify different kinds of analyses without going into the technicalities of the implementation.

In this demonstration, we introduce a system that tries to meet precisely the above needs. Running on streaming data, the system provides an abstraction which allows the user to specify real time events in the stream, for which he wishes to be notified. Also, acting as a data-store for the tweet network, the system provides another abstraction which allows the user to formulate complex queries on this historical data. We demonstrate both of these abstractions using an example of each, on real world data.

Here we provide a comprehenstive documentaion of each component of the system along with a documentation of the code.


\chapter{Introduction to twitter analytics system}
\label{\detokenize{introduction:introduction-to-twitter-analytics-system}}\label{\detokenize{introduction::doc}}

\chapter{Read data from twitter streaming API}
\label{\detokenize{twitter_stream:read-data-from-twitter-streaming-api}}\label{\detokenize{twitter_stream::doc}}

\chapter{Ingesting data into Neo4j}
\label{\detokenize{neo4j_data_ingestion:ingesting-data-into-neo4j}}\label{\detokenize{neo4j_data_ingestion::doc}}

\section{Code Documentation}
\label{\detokenize{neo4j_data_ingestion:code-documentation}}

\chapter{Ingesting data into MongoDB}
\label{\detokenize{mongoDB_data_ingestion:ingesting-data-into-mongodb}}\label{\detokenize{mongoDB_data_ingestion::doc}}

\section{Why store in MongoDB}
\label{\detokenize{mongoDB_data_ingestion:why-store-in-mongodb}}
In mongoDB we store only the data which can be extracted quickly from incoming tweets without much processing.

This means that any query which can be answered using mongoDB can also be answered using the network data in neo4j. This has been done to ensure that some very common queries can be answered quickly. Also, neo4j has a limit on the parallel sessions that can be made to the database, so in case we decide to do away with mongoDB, those queries would have to be answered from neo4j and would unnecesarily take up the sessions.


\section{Data Format in mongoDB}
\label{\detokenize{mongoDB_data_ingestion:data-format-in-mongodb}}
We have three collections in mongoDB:
\begin{itemize}
\item {} \begin{description}
\item[{To store the hashtags. Each document in this collection stores the following infomration:}] \leavevmode\begin{itemize}
\item {} 
the hashtag

\item {} 
the timestamp of the tweet which contained the hashtag

\item {} 
the sentiment associated with the tweet containing the hashtag

\end{itemize}

\end{description}

\item {} \begin{description}
\item[{To store urls}] \leavevmode\begin{itemize}
\item {} 
the url

\item {} 
the timestamp of the tweet which contained the hashtag

\item {} 
the sentiment associated with the tweet containing the hashtag

\end{itemize}

\end{description}

\item {} \begin{description}
\item[{To store user mentions}] \leavevmode\begin{itemize}
\item {} 
the user mention

\item {} 
the timestamp of the tweet which contained the hashtag

\item {} 
the sentiment associated with the tweet containing the hashtag

\end{itemize}

\end{description}

\end{itemize}

Given this information in mongoDB, we can currently use it to answer queries like:
\begin{itemize}
\item {} 
Most popular hashtags(and their sentiment) in total

\item {} 
Most popular hashtags(and their sentiment) in an interval of time

\item {} 
Most popular urls in total

\item {} 
Most popular urls in an interval of time

\item {} 
Most popular users in total(in terms of their mentions)

\item {} 
Most popular users in an interval of time(in terms of their mentions)

\end{itemize}


\section{mongoDB v/s neo4j}
\label{\detokenize{mongoDB_data_ingestion:mongodb-v-s-neo4j}}
Note that just the bare minimum information that is currently being stored in the mongoDB. It can easily be extended to store more information. MongoDB provides strong mechanisms to aggregate and extract information
from the database.

So, even if we decide to store some pseudo-structural information, like the user of the tweet in hashtags collection and then answer queries like the sentiment associated will all the tweets of an user, we expect the query execution time to be atleast as fast as answering the query in neo4j, though in case of neo4j also, answering such query would also take only a single hop, which means that the execution time would be small anyways. This is precisely the reason why we don’t currently store such information in mongoDB.

But, as the size of the system grows, it would surely be benefitial to store much more condensed data in mongoDB and use it to answer more complex queries.


\section{Ingesting the data into database}
\label{\detokenize{mongoDB_data_ingestion:ingesting-the-data-into-database}}
A simple approach would be to ingest a tweet into the database as when it comes in real time. But clearly(and as mentioned in mongoDB documentation)
this is suboptimal, as we are connecting to the on-disk database frequently.{[}scheme 1{]}

An easy solution to this would be to keep collecting the data in memory and then write it to the database periodically. But observe that, the time it takes the process to open a connection to database and then write the data to it, no new tweeets are being collected in memory.{[}scheme 2{]}

So finally we the approach of utilising multiple processes to write data to mongoDB.{[}scheme 3{]}

Observe here the distinction between a thread and a process.
While using multiple threads, the threads are run(usually, if we discount the kernel threads spawned by python) on a single core in python, due to Global Interpreter Lock and thus, though we get virtual parallelism,
we don’t get real parallelism. Thus, due to the limitation of the language, we are using process to get the parallelism between wriing to database and collecitng new tweets.
A clear disadvantage of using process over threads will become clear below.

To explain the final multi-process approach, we have three processes running:
\begin{itemize}
\item {} 
Accumulator process - It collects the tweets in an in-memory data structure. Also, in the begining at t=0, it spawns a timer thread, which generates an interrupt after every pre-specified T time.

\item {} 
Connector process - It takes a list of tweets through a pipe, opens connection to the database and writes the tweets to the database.

\end{itemize}

How the system works can be understood through this image:

\noindent\sphinxincludegraphics{{mongo_ingestion}.jpg}

So, the timer process in the accumulator process genertes an interrupt after every T seconds, at this instant, the accumulator stops collecting tweets and writes those to Inter process communication(IPC) pipe. This is generally fast as IPC pipe are implemented in memory. Now, the other end of the pipe is in the connector process. After the writing process has been complete, it recieves the tweets and starts writing those to the on-disk database as a batch, which again ensures that the process is faster as compared to writing single tweet at a time in a loop. Concurrently, while the connector process is writing the tweets, the acuumulator process starts accumulating new tweets.

So in this way the the process of writing to database in connector process is overlapped with the the accumulation of tweets in accumulator process. Note that we have a small gap equivalent to time taken to write to IPC, in which the accummulator process is not collecting the tweets. The whole process can further be made efficient by removing this gap, but since we are getting tweet ingestion rate much more than the rate of tweets coming on twitter and the gain from removing the gap would not be much, we don’t implement it.

To answer queries like the most popular hastags in total, or most popular hashtags in a large interval. It would be benefitial to have aggregates over a larger interval. For example, say we want to get the most popular hashtags in an year, it would be helpful in that setting to have an aggregated document containing 100 most popular hashtags in each month, then we can consider a union of these 12 documents plus some counting from the interval edges to get the most popular hashtags. Clearly, this will fasten the query answering rate. Though, this would not always give the exactly accurate results and can also not be used to get the counts of hashtags, but can be used to get most popular k hashtags as the size of data grows. To implement it, simply spawn another thread in the connector process to read data from the hashtags collection at a specific time interval(like 1 week), aggregate the data and store the aggregated information into a new collection. We provide the code for this, but don’t currently use this mechanism.


\section{Ingestion Rates}
\label{\detokenize{mongoDB_data_ingestion:ingestion-rates}}
As expected, the ingestion rate into mongoDB whilw overlapping writing into database and accumulating data is faster than without parallelization. The plot below shows a comparison between scheme 2 and scheme 3 as described above. Observe that as more and more tweets are inserted, the difference between the two scheme grows as the time saved in overlapping inserting the accumulating keeps on adding up in advntage of scheme 3.

\noindent\sphinxincludegraphics{{image1}.png}

Clearly the ingestion rate depends on the time after which the interrupt to start write the collected tweets to database is generate(called T above).

Finally we get an ingestion rate of around 7k-12k tweets/second on average, depending on T.


\section{Code Documentation}
\label{\detokenize{mongoDB_data_ingestion:module-ingest_raw}}\label{\detokenize{mongoDB_data_ingestion:code-documentation}}\index{ingest\_raw (module)}\index{Ingest (class in ingest\_raw)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Ingest}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{ingest\_raw.}}\sphinxbfcode{\sphinxupquote{Ingest}}}{\emph{interval}}{}
Bases: \sphinxcode{\sphinxupquote{object}}
\index{aggregate() (ingest\_raw.Ingest method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Ingest.aggregate}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{aggregate}}}{}{}
\end{fulllineitems}

\index{exit() (ingest\_raw.Ingest method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Ingest.exit}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{exit}}}{}{}
\end{fulllineitems}

\index{insert\_tweet() (ingest\_raw.Ingest method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Ingest.insert_tweet}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{insert\_tweet}}}{\emph{tweet}}{}
update the in memory dictionaries

\end{fulllineitems}

\index{populate() (ingest\_raw.Ingest method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Ingest.populate}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{populate}}}{}{}
write to the mongoDB

\end{fulllineitems}

\index{worker() (ingest\_raw.Ingest method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Ingest.worker}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{worker}}}{\emph{q}}{}
\end{fulllineitems}


\end{fulllineitems}

\index{Timer (class in ingest\_raw)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Timer}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{ingest\_raw.}}\sphinxbfcode{\sphinxupquote{Timer}}}{\emph{interval}, \emph{function}, \emph{args=None}, \emph{kwargs=None}, \emph{iterations=1}, \emph{infinite=False}}{}
Bases: \sphinxcode{\sphinxupquote{multiprocessing.context.Process}}

Calls a function after a specified number of seconds:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{t} \PYG{o}{=} \PYG{n}{Timer}\PYG{p}{(}\PYG{l+m+mf}{30.0}\PYG{p}{,} \PYG{n}{f}\PYG{p}{,} \PYG{n}{args}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,} \PYG{n}{kwargs}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{t}\PYG{o}{.}\PYG{n}{start}\PYG{p}{(}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{t}\PYG{o}{.}\PYG{n}{cancel}\PYG{p}{(}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{}stops the timer if it is still waiting}
\end{sphinxVerbatim}
\index{authkey (ingest\_raw.Timer attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Timer.authkey}}\pysigline{\sphinxbfcode{\sphinxupquote{authkey}}}
\end{fulllineitems}

\index{cancel() (ingest\_raw.Timer method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Timer.cancel}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cancel}}}{}{}
Stop the timer if it hasn’t already finished.

\end{fulllineitems}

\index{daemon (ingest\_raw.Timer attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Timer.daemon}}\pysigline{\sphinxbfcode{\sphinxupquote{daemon}}}
Return whether process is a daemon

\end{fulllineitems}

\index{exitcode (ingest\_raw.Timer attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Timer.exitcode}}\pysigline{\sphinxbfcode{\sphinxupquote{exitcode}}}
Return exit code of process or \sphinxtitleref{None} if it has yet to stop

\end{fulllineitems}

\index{ident (ingest\_raw.Timer attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Timer.ident}}\pysigline{\sphinxbfcode{\sphinxupquote{ident}}}
Return identifier (PID) of process or \sphinxtitleref{None} if it has yet to start

\end{fulllineitems}

\index{is\_alive() (ingest\_raw.Timer method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Timer.is_alive}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{is\_alive}}}{}{}
Return whether process is alive

\end{fulllineitems}

\index{join() (ingest\_raw.Timer method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Timer.join}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{join}}}{\emph{timeout=None}}{}
Wait until child process terminates

\end{fulllineitems}

\index{name (ingest\_raw.Timer attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Timer.name}}\pysigline{\sphinxbfcode{\sphinxupquote{name}}}
\end{fulllineitems}

\index{pid (ingest\_raw.Timer attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Timer.pid}}\pysigline{\sphinxbfcode{\sphinxupquote{pid}}}
Return identifier (PID) of process or \sphinxtitleref{None} if it has yet to start

\end{fulllineitems}

\index{run() (ingest\_raw.Timer method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Timer.run}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{run}}}{}{}
Method to be run in sub-process; can be overridden in sub-class

\end{fulllineitems}

\index{sentinel (ingest\_raw.Timer attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Timer.sentinel}}\pysigline{\sphinxbfcode{\sphinxupquote{sentinel}}}
Return a file descriptor (Unix) or handle (Windows) suitable for
waiting for process termination.

\end{fulllineitems}

\index{start() (ingest\_raw.Timer method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Timer.start}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{start}}}{}{}
Start child process

\end{fulllineitems}

\index{terminate() (ingest\_raw.Timer method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Timer.terminate}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{terminate}}}{}{}
Terminate process; sends SIGTERM signal or uses TerminateProcess()

\end{fulllineitems}


\end{fulllineitems}

\index{calculate\_sentiment() (in module ingest\_raw)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.calculate_sentiment}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{ingest\_raw.}}\sphinxbfcode{\sphinxupquote{calculate\_sentiment}}}{\emph{positive\_words}, \emph{negative\_words}, \emph{tweet\_text}}{}
\end{fulllineitems}

\index{getDateFromTimestamp() (in module ingest\_raw)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.getDateFromTimestamp}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{ingest\_raw.}}\sphinxbfcode{\sphinxupquote{getDateFromTimestamp}}}{\emph{timestamp}}{}
\end{fulllineitems}

\index{read\_tweets() (in module ingest\_raw)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.read_tweets}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{ingest\_raw.}}\sphinxbfcode{\sphinxupquote{read\_tweets}}}{\emph{path}, \emph{filename}}{}
\end{fulllineitems}

\index{threaded() (in module ingest\_raw)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.threaded}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{ingest\_raw.}}\sphinxbfcode{\sphinxupquote{threaded}}}{\emph{fn}}{}
\end{fulllineitems}



\chapter{Neo4j: API to generate cypher queries}
\label{\detokenize{neo4j_query_generation:neo4j-api-to-generate-cypher-queries}}\label{\detokenize{neo4j_query_generation::doc}}
Here we expalin the API to generate cypher queries for Neo4j.


\section{Template of a general query}
\label{\detokenize{neo4j_query_generation:template-of-a-general-query}}
Any query can be thought of as a 2 step process -
\begin{itemize}
\item {} 
Extract the relevant sub-graph satisfying the query constraints (Eg. Users and their tweets that use a certain hashtag)

\item {} 
Post-processing of this sub-graph to return desired result (Eg. Return “names” of such users, Return “number” of such users)

\end{itemize}

In a generic way, the 1st step can be constructed using AND,OR,NOT of multiple constraints. We now specify how each such constraint can be built.

We look at the network in an abstract in two dimensions.
\begin{itemize}
\item {} 
There are “Entities” (users and tweets) which have “Attributes” (like user has screen\_name,follower\_count etc. and tweet has hashtag,mentions etc.).

\item {} 
The entities have “Relations” between them which have the only attribute as time/time-interval (Eg. Follows “relation” between 2 user “entities” has a time-interval associated).

\end{itemize}

So each constraint can be specified by specifying a pattern consisting of
\begin{itemize}
\item {} 
Two Entities and their Attributes

\item {} 
Relation between the entities and its Attribute (which is the time constraint of this relation)

\end{itemize}

To make things clear we provide an example here.
Suppose our query is - Find users who follow a user with id=1 and have also tweeted with a hashtag “h” between time t1 and t2.
We first break this into AND of two constraints:
\begin{itemize}
\item {} 
User follows a user with id=1

\item {} 
User has tweeted with a hashtag “h” between time t1 and t2.

\end{itemize}

We now specify the 1st constraint using our entity-attribute abstraction.
\begin{itemize}
\item {} 
Source entity - User, Attributes - None

\item {} 
Destination entity - User, Attributes - id=1

\item {} 
Relationship - Follows, Attributes - None

\end{itemize}

We now specify the 2nd constraint using our entity-attribute abstraction.
\begin{itemize}
\item {} 
Source entity - User, Attributes - None

\item {} 
Destination entity - Tweet, Attributes - hashtag:”h”

\item {} 
Relationship - Follows, Attributes - b/w t1,t2

\end{itemize}

The missing thing in this abstraction is that we have not taken into account that the source entity in both the constraints refers to the same User. To do so, we “name” each entity (like a variable). So we have:
\begin{itemize}
\item {} \begin{description}
\item[{Constraint 1:}] \leavevmode\begin{itemize}
\item {} 
Source entity - u1:User, Attributes - None

\item {} 
Destination entity - u2:User, Attributes - id=1

\item {} 
Relationship - Follows, Attributes - None

\end{itemize}

\end{description}

\item {} \begin{description}
\item[{Constraint 2:}] \leavevmode\begin{itemize}
\item {} 
Source entity - u1:User, Attributes - None

\item {} 
Destination entity - u3:Tweet, Attributes - hashtag:”h”

\item {} 
Relationship - Follows, Attributes - b/w t1,t2

\end{itemize}

\end{description}

\end{itemize}


\section{Creating a custom query through dashboard API : Behind the scenes}
\label{\detokenize{neo4j_query_generation:creating-a-custom-query-through-dashboard-api-behind-the-scenes}}
A user can follow the general template of a query as provided above to build a query.
when a user provides the inputs to specify the query, the following steps are executed on the server:
\begin{itemize}
\item {} 
Cleanup and processing of the inputs provided by the user.

\item {} 
The variables(User/Tweet) and the relations are stored in a database. These stored objects can be later used by the user.

\item {} 
The query specified by the user is converted into a Cypher neo4j graph mining query.

\item {} 
Connection is established with the neo4j server and the query is executed on the database.

\item {} 
The results obtained are concatenated and are displayed.

\end{itemize}


\section{Code Documentation}
\label{\detokenize{neo4j_query_generation:code-documentation}}
Here we provide a documentation of the code.

\phantomsection\label{\detokenize{neo4j_query_generation:module-generate_queries}}\index{generate\_queries (module)}
Module to generate cypher code for inputs taken from user though dashboard API.

The {\hyperref[\detokenize{neo4j_query_generation:module-generate_queries}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{generate\_queries}}}}} module contains the classes:
\begin{itemize}
\item {} 
{\hyperref[\detokenize{neo4j_query_generation:generate_queries.CreateQuery}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{generate\_queries.CreateQuery}}}}}

\end{itemize}

One can use the {\hyperref[\detokenize{neo4j_query_generation:generate_queries.CreateQuery.create_query}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{generate\_queries.CreateQuery.create\_query()}}}}} to build a cypher query.

Example illustrating how to create a query which gives the userids and their tweet counts who have used a certian hashtag.

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{actors}\PYG{o}{=}\PYG{p}{[}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{u}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{USER}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{t}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{TWEET}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{t1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{TWEET}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{attributes}\PYG{o}{=}\PYG{p}{[}\PYG{p}{[}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{hashtag}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}hash\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{p}{]}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{relations}\PYG{o}{=}\PYG{p}{[}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{u}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{TWEETED}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{t}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{u}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{TWEETED}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{t1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{cq} \PYG{o}{=} \PYG{n}{CreateQuery}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{return\PYGZus{}values}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{u.id,count(t1)}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{ret\PYGZus{}dict} \PYG{o}{=} \PYG{n}{cq}\PYG{o}{.}\PYG{n}{create\PYGZus{}query}\PYG{p}{(}\PYG{n}{actors}\PYG{p}{,}\PYG{n}{attributes}\PYG{p}{,}\PYG{n}{relations}\PYG{p}{,}\PYG{n}{return\PYGZus{}values}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{pprint}\PYG{p}{(}\PYG{n}{ret\PYGZus{}dict}\PYG{p}{,}\PYG{n}{width}\PYG{o}{=}\PYG{l+m+mi}{150}\PYG{p}{)}
\end{sphinxVerbatim}

Example of a query which uses time indexing in a relationship:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{actors} \PYG{o}{=} \PYG{p}{[}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{USER}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,} \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{u1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{USER}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]} \PYG{o}{+} \PYG{p}{[}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{t1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{TWEET}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,} \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{t2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{TWEET}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{attributes} \PYG{o}{=} \PYG{p}{[}\PYG{p}{[}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{12}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{24}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}\PYG{p}{]}\PYG{o}{+}\PYG{p}{[}\PYG{p}{[}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{hashtag}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BLUERISING}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{retweet\PYGZus{}of}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{t1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,} \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{has\PYGZus{}mention}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{u1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{relations} \PYG{o}{=} \PYG{p}{[}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{FOLLOWS}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{u1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,} \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{TWEETED}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{t2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{24}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{48}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}
\index{CreateQuery (class in generate\_queries)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neo4j_query_generation:generate_queries.CreateQuery}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{generate\_queries.}}\sphinxbfcode{\sphinxupquote{CreateQuery}}}
Bases: \sphinxcode{\sphinxupquote{object}}

Class containing functions to generate query.
\index{conditional\_create() (generate\_queries.CreateQuery method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neo4j_query_generation:generate_queries.CreateQuery.conditional_create}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{conditional\_create}}}{\emph{entity}}{}
Condionally provide the attributes of the node if not already created, else directly use the name of the variable create earlier.
If already create, pass empty list of properties.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{entity}} \textendash{} the entity which to check and create

\item[{Returns}] \leavevmode
the code for the node as neo4j node enclosed in ()

\end{description}\end{quote}

\end{fulllineitems}

\index{create\_query() (generate\_queries.CreateQuery method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neo4j_query_generation:generate_queries.CreateQuery.create_query}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{create\_query}}}{\emph{actors}, \emph{attributes}, \emph{relations}, \emph{return\_values}}{}
Takes a list of attributes and relationships between them and return a cypher code as string.
For the format of the lists see the examples.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{actors}} \textendash{} the variable names, types of the attributes

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{attributes}} \textendash{} the properties of the actors

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{relations}} \textendash{} the relations between the entities along with time index for the relations

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{return\_values}} \textendash{} a direct string containing the return directive.

\end{itemize}

\item[{Returns}] \leavevmode
index of the bond in the molecule

\end{description}\end{quote}

\begin{sphinxadmonition}{note}{Note:}
Here we are expecting that if user has not specified the times on the dashboard, then we pass epmty string. If you
store some other default in dashboard database then change this accordingly.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Note:}
The return \_values is directly used as a string in the cypher query, so the user can use AS and other similar cypher directives while specifying the query.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{\label{neo4j_query_generation:index-0}Todo:}
Support to compose queries using OR. For example, currently compostion of relationships or attribute properties like all tweets(t) which are retweets of t1 or quoted t2, is not supported. Use cypher union for this.
\end{sphinxadmonition}

\end{fulllineitems}

\index{generate\_node() (generate\_queries.CreateQuery method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neo4j_query_generation:generate_queries.CreateQuery.generate_node}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{generate\_node}}}{\emph{var}, \emph{type}, \emph{props}}{}
Helper function for {\hyperref[\detokenize{neo4j_query_generation:generate_queries.CreateQuery.conditional_create}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{generate\_queries.CreateQuery.conditional\_create()}}}}}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{var}} \textendash{} the variable name of the entity

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{type}} \textendash{} the type of the entity. Observe we pass type as :USER and NOT as USER

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{props}} \textendash{} the properties of the entity.

\end{itemize}

\item[{Returns}] \leavevmode
the code for the node as neo4j node enclosed in ()

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\chapter{Generating queries in mongoDB}
\label{\detokenize{mongoDB_query_generation:generating-queries-in-mongodb}}\label{\detokenize{mongoDB_query_generation::doc}}

\chapter{About postprocessing functions}
\label{\detokenize{postprocessing:about-postprocessing-functions}}\label{\detokenize{postprocessing::doc}}

\chapter{Composing multiple queries : DAG}
\label{\detokenize{dag:composing-multiple-queries-dag}}\label{\detokenize{dag::doc}}

\section{Basic terminology}
\label{\detokenize{dag:basic-terminology}}
When we say \sphinxstylestrong{Query}, it means an one of the following three things:
\begin{itemize}
\item {} 
MongoDB query : A query not capable of giving any network information

\item {} 
Neo4j query : A network based and/or time indexed query on the twitter network

\item {} 
Post processing function : A python function which takes outups of query(ies) as inputs and transforms them to give the output

\end{itemize}

\sphinxstylestrong{DAG} stands for directed acylic graph. Thus it a directed graph with no cycles. The idea behind a DAG is to compose mutiple queries to build a complex queries. A DAG has nodes and has directed connections connections between the nodes. The nodes represent queries.


\section{Idea behind a DAG}
\label{\detokenize{dag:idea-behind-a-dag}}
As mentioned above, our main idea is to provie the user an easy abstraction to build complex queries. But apart from this there are several functions that the abstraction of a DAG seems to serve, which we list below:
\begin{itemize}
\item {} 
Provide an abstraction to build complex queires from simple queries.

\item {} 
A particular database may be suited to answer particular type of queries. In fact this is the root reason behind storing data in mongoDB to answer commonly encountered queries. We expect the user to have a basic understanding of the database schemas and thus be able to have an idea of efficiency of the two databases in answering specific queries. Having such knowledge, the user can compose different queries in sake of efficiency.

\item {} 
It may be easy to do some projection on data output by a query post the execution, rather than coding it in the cypher in case of neo4j, or the aggregation pipeline in case of mongoDB. Thus, given the DAG abstraction, the user can feed te output of the query into a postprocessing node.

\item {} 
On similar lines as above, the user may need to aggregate multiple outputs from different queries in a postprocessing function in a custom manner not supported by the query mechanism of the databases.

\item {} 
Breaking a big query into smaller ones may be benefitial from the end user point of view because by doing so we can show the incremental results of the smaller parts to the user instead of waiting for the entire big query to execute.

\end{itemize}

In this abstraction, a single query can also be treated as a DAG, one having a single node and no connections.

We store the queries that the user creates through the dashboard. The user can then specify the structure of the DAG network by uploading a file in which he specifies how ouputs and inputs of queries are connected. We provide the details in the next section.


\section{Building a DAG from queries}
\label{\detokenize{dag:building-a-dag-from-queries}}
A DAG is composition of queries in which we need to specify how the outputs of queries downstrea feed into the inputs of the upstream ones.

We explain how to build the queries with the help on an example. Let us build a DAG to get the most active users. Refer to this image(the green queries represent mongoDB queries and blue ones represent neo4j queries):

\noindent\sphinxincludegraphics{{example_query}.jpg}

First we need to build the three queries separately, let us say we have the built queries as:
\begin{itemize}
\item {} \begin{description}
\item[{mongoDB query(most\_popular\_hashtags\_20 - Node 1) - 20 most popular hashtags in total}] \leavevmode\begin{itemize}
\item {} 
INPUTS  : limit(number of records to return)

\item {} 
OUTPUTS : hashtags(list of popular hashtags, arranged by count in decreasing order), counts(list of their corresponding counts)

\end{itemize}

\end{description}

\item {} \begin{description}
\item[{mongoDB query(most\_popular\_mentions\_20 - Node 2) - 20 most popular users(in terms of number of mentions) in total}] \leavevmode\begin{itemize}
\item {} 
INPUTS  : limit(number of records to return)

\item {} 
OUTPUTS : user\_metions(list of popular users, arranged by count in decreasing order), counts(list of their corresponding counts)

\end{itemize}

\end{description}

\item {} \begin{description}
\item[{neo4j query(active\_users - Node 3)   - userIds and their tweet counts who have used one of the popular hashtags atleast once and have tweeted with one of the popular user mentions atleast once}] \leavevmode\begin{itemize}
\item {} 
INPUTS  : hash\_in(list of 20 most popular hashtags), users\_in(list of 20 most popular users)

\item {} 
OUTPUTS : userIds(list of required users), tweet\_counts(total number of their tweets)

\end{itemize}

\end{description}

\end{itemize}

This query is demonstrated by the block diagram below also:

\noindent\sphinxincludegraphics{{example_query_detailed}.jpg}

As mentioned in neo4j query generation section, we expect all the inputs to the neo4j query to be  list of native objects. We put a similar constraint on the inputs to post processing function. Keeping this in mind, to ensure consistency and a seamless flow of information, the outputs of each query(mongoDB, neo4j or postprocessing function) is expected to be a list. Thus each node in the DAG accepts a dictionary as input in which the keys are lists and similarly returns a dictionary with list values. The keys in both dictioanry is the name of hte inputs/outputs, as specified in the query generation.

The only place where the list input breaks is in case of mongoDB query as they require some basic inputs which can directly be provided as native objects(for example the limit input to the above two mongoDB queries).

Further we need to specify which outputs of the queries are to be returned.

The example input file to create the above DAG looks something like this:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+m+mi}{3}
\PYG{n}{n1} \PYG{n}{most\PYGZus{}popular\PYGZus{}hashtags\PYGZus{}20}
\PYG{n}{n2} \PYG{n}{most\PYGZus{}popular\PYGZus{}mentions\PYGZus{}20}
\PYG{n}{n3} \PYG{n}{active\PYGZus{}users}
\PYG{n}{INPUTS}\PYG{p}{:}
\PYG{n}{CONNECTIONS}\PYG{p}{:}
\PYG{n}{n1}\PYG{o}{.}\PYG{n}{hashtag} \PYG{n}{n3}\PYG{o}{.}\PYG{n}{hashtag}
\PYG{n}{n2}\PYG{o}{.}\PYG{n}{userId} \PYG{n}{n3}\PYG{o}{.}\PYG{n}{um\PYGZus{}id}
\PYG{n}{RETURNS}\PYG{p}{:}
\PYG{n}{n3}\PYG{o}{.}\PYG{n}{userId}
\PYG{n}{n3}\PYG{o}{.}\PYG{n}{count}
\end{sphinxVerbatim}


\section{DAG in airflow}
\label{\detokenize{dag:dag-in-airflow}}
Similary we generate the code to specify the dag in airflow something like this.

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{task\PYGZus{}0} \PYG{o}{=} \PYG{n}{PythonOperator}\PYG{p}{(}
    \PYG{n}{task\PYGZus{}id}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{node\PYGZus{}\PYGZob{}\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{n1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
    \PYG{n}{python\PYGZus{}callable}\PYG{o}{=}\PYG{n}{execute\PYGZus{}query}\PYG{p}{,}
    \PYG{n}{op\PYGZus{}kwargs}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{node\PYGZus{}name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{n1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
    \PYG{n}{provide\PYGZus{}context} \PYG{o}{=} \PYG{n+nb+bp}{True}\PYG{p}{,}
    \PYG{n}{dag}\PYG{o}{=}\PYG{n}{dag}\PYG{p}{)}

\PYG{n}{task\PYGZus{}1} \PYG{o}{=} \PYG{n}{PythonOperator}\PYG{p}{(}
        \PYG{n}{task\PYGZus{}id}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{node\PYGZus{}\PYGZob{}\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{n2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
        \PYG{n}{python\PYGZus{}callable}\PYG{o}{=}\PYG{n}{execute\PYGZus{}query}\PYG{p}{,}
        \PYG{n}{op\PYGZus{}kwargs}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{node\PYGZus{}name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{n2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
        \PYG{n}{provide\PYGZus{}context} \PYG{o}{=} \PYG{n+nb+bp}{True}\PYG{p}{,}
        \PYG{n}{dag}\PYG{o}{=}\PYG{n}{dag}\PYG{p}{)}

\PYG{n}{task\PYGZus{}2} \PYG{o}{=} \PYG{n}{PythonOperator}\PYG{p}{(}
        \PYG{n}{task\PYGZus{}id}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{node\PYGZus{}\PYGZob{}\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{n3}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
        \PYG{n}{python\PYGZus{}callable}\PYG{o}{=}\PYG{n}{execute\PYGZus{}query}\PYG{p}{,}
        \PYG{n}{op\PYGZus{}kwargs}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{node\PYGZus{}name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{n3}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
        \PYG{n}{provide\PYGZus{}context} \PYG{o}{=} \PYG{n+nb+bp}{True}\PYG{p}{,}
        \PYG{n}{dag}\PYG{o}{=}\PYG{n}{dag}\PYG{p}{)}
\PYG{n}{task\PYGZus{}0} \PYG{o}{\PYGZgt{}\PYGZgt{}} \PYG{n}{task\PYGZus{}2}
\PYG{n}{task\PYGZus{}1} \PYG{o}{\PYGZgt{}\PYGZgt{}} \PYG{n}{task\PYGZus{}2}
\end{sphinxVerbatim}

In the above code, the execute query is the function in which we execute queries and pass on their outputs to XComs to be used by the downstream nodes.

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Pushing onto XComs}
\PYG{n}{context}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{task\PYGZus{}instance}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{xcom\PYGZus{}push}\PYG{p}{(}\PYG{n}{k}\PYG{p}{,}\PYG{n}{v}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} Pulling from XComs}
\PYG{n}{context}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{task\PYGZus{}instance}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{xcom\PYGZus{}pull}\PYG{p}{(}\PYG{n}{task\PYGZus{}ids}\PYG{o}{=}\PYG{n}{get\PYGZus{}task\PYGZus{}from\PYGZus{}node}\PYG{p}{(}\PYG{n}{mapp}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dag\PYGZus{}id} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{active\PYGZus{}users\PYGZus{}dag}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{key}\PYG{o}{=}\PYG{n}{k}\PYG{p}{)}
\end{sphinxVerbatim}


\section{Creating custom metric}
\label{\detokenize{dag:creating-custom-metric}}
Custom metric can be created on top of the DAG.


\chapter{Generating alerts using flink and kafka}
\label{\detokenize{flink:generating-alerts-using-flink-and-kafka}}\label{\detokenize{flink::doc}}

\chapter{Benchmarking the query answering}
\label{\detokenize{benchmarking:benchmarking-the-query-answering}}\label{\detokenize{benchmarking::doc}}

\chapter{Dashboard Website}
\label{\detokenize{dashboard_website:dashboard-website}}\label{\detokenize{dashboard_website::doc}}

\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\DUrole{xref,std,std-ref}{search}

\end{itemize}


\renewcommand{\indexname}{Python Module Index}
\begin{sphinxtheindex}
\def\bigletter#1{{\Large\sffamily#1}\nopagebreak\vspace{1mm}}
\bigletter{g}
\item {\sphinxstyleindexentry{generate\_queries}}\sphinxstyleindexpageref{neo4j_query_generation:\detokenize{module-generate_queries}}
\indexspace
\bigletter{i}
\item {\sphinxstyleindexentry{ingest\_raw}}\sphinxstyleindexpageref{mongoDB_data_ingestion:\detokenize{module-ingest_raw}}
\end{sphinxtheindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}